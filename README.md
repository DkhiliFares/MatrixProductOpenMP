# ðŸ”¢ Parallel Matrix Multiplication with OpenMP

This C++ project demonstrates different approaches to matrix multiplication using **OpenMP** for parallel computing. It compares the performance of sequential and parallel implementations across various loop configurations.

---

## ðŸš€ Features

- Matrix generation with random floating-point values
- **Sequential matrix multiplication**
- **Parallel multiplication** using OpenMP:
  - Outer loop parallelization
  - Middle loop parallelization
  - Outer and middle loops combined (`collapse(2)`)

---

## ðŸ› ï¸ Technologies

- C++
- OpenMP
- Standard Template Library (`vector`)
- Random number generation
- Performance benchmarking (`omp_get_wtime`, `clock`)

---

## ðŸ“ˆ Performance Evaluation

The program prints the execution time for:

- âœ… Sequential execution
- ðŸ§µ Parallel (outer loop)
- ðŸ§µ Parallel (middle loop)
- ðŸ§µ Parallel (outer + middle loop with collapse)

This allows easy comparison of efficiency gains from different parallel strategies.

---

## âš™ï¸ How to Compile

Make sure OpenMP is enabled in your compiler:

```bash
g++ -fopenmp -O2 matrix_multiplication.cpp -o matrix_multiplication
```

Then run:

```bash
./matrix_multiplication
```

---

## ðŸ“Œ Example Output

```
nombre max de threads egale a 8
Parallel outer loop execution time: 3.51 seconds
Parallel middle loop execution time: 3.78 seconds
Parallel outer and middle loops execution time: 2.89 seconds
Sequential execution time: 15.62 seconds
```

> â±ï¸ Actual results depend on system specs and matrix size.

---

## ðŸ“š Notes

- Default matrix size: `1024 x 1024`
- Modify the `N` variable in `main()` to test with different sizes.
- Matrix multiplication is computationally intensive: larger sizes benefit more from parallelization.

---

## ðŸ§  Author

**Fares Dkhili**  
ðŸ”— [GitHub](https://github.com/DkhiliFares) | ðŸ“« dkhili.fares@gmail.com

---

> *Optimizing performance through parallelism â€” one loop at a time.*
```


